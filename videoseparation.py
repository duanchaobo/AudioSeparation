import os
import threading
import queue
from datetime import timedelta, datetime
import gradio as gr
import torch.cuda
from pydub import AudioSegment
from pydub.silence import detect_nonsilent
import ffmpeg
import psutil
import numpy as np
import json
import re
import wave
import contextlib

spk_txt_queue = queue.Queue()
audio_concat_queue = queue.Queue()

# åˆå§‹åŒ–å…¨å±€å˜é‡
home_directory = os.path.expanduser("~")
asr_model_path = os.path.join(home_directory, ".cache", "modelscope", "hub", "models", "iic", "speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch")
asr_model_revision = "v2.0.4"
vad_model_path = os.path.join(home_directory, ".cache", "modelscope", "hub", "models", "iic", "speech_fsmn_vad_zh-cn-16k-common-pytorch")
vad_model_revision = "v2.0.4"
punc_model_path = os.path.join(home_directory, ".cache", "modelscope", "hub", "models", "iic", "punc_ct-transformer_zh-cn-common-vocab272727-pytorch")
punc_model_revision = "v2.0.4"
spk_model_path = os.path.join(home_directory, ".cache", "modelscope", "hub", "models", "iic", "speech_campplus_sv_zh-cn_16k-common")
spk_model_revision = "v2.0.4"
hotword_file = "./hotwords.txt"

# åªæ”¯æŒè§†é¢‘æ ¼å¼
support_video_format = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.ts']
support_subtitle_format = ['.srt']

# åŠ è½½æ¨¡å‹ï¼ˆå…¨å±€åªåŠ è½½ä¸€æ¬¡ï¼‰
device = "cuda" if torch.cuda.is_available() else "cpu"
ngpu = 1 if device == "cuda" else 0
ncpu = psutil.cpu_count(logical=False)  # ä½¿ç”¨ç‰©ç†æ ¸å¿ƒæ•°

print(f"ä½¿ç”¨è®¾å¤‡: {device}, GPUæ•°é‡: {ngpu}, CPUæ ¸å¿ƒæ•°: {ncpu}")

# ASR æ¨¡å‹
model = None
def load_model():
    global model
    from funasr import AutoModel
    model = AutoModel(
        model=asr_model_path,
        model_revision=asr_model_revision,
        vad_model=vad_model_path,
        vad_model_revision=vad_model_revision,
        punc_model=punc_model_path,
        punc_model_revision=punc_model_revision,
        spk_model=spk_model_path,
        spk_model_revision=spk_model_revision,
        ngpu=ngpu,
        ncpu=ncpu,
        device=device,
        disable_pbar=True,
        disable_log=True,
        disable_update=True
    )
    print("æ¨¡å‹åŠ è½½å®Œæˆ")

# åœ¨åå°çº¿ç¨‹åŠ è½½æ¨¡å‹
threading.Thread(target=load_model).start()

# çƒ­è¯å¤„ç†
hotwords = ''
if os.path.exists(hotword_file):
    with open(hotword_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
        lines = [line.strip() for line in lines]
    hotwords = " ".join(lines)
    print(f"åŠ è½½çƒ­è¯: {hotwords}")

def to_date(milliseconds):
    """å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºSRTæ ¼å¼çš„æ—¶é—´"""
    time_obj = timedelta(milliseconds=milliseconds)
    return f"{time_obj.seconds // 3600:02d}:{(time_obj.seconds // 60) % 60:02d}:{time_obj.seconds % 60:02d}.{time_obj.microseconds // 1000:03d}"

def to_milliseconds(time_str):
    """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ¯«ç§’"""
    try:
        # æ ‡å‡†åŒ–æ—¶é—´å­—ç¬¦ä¸²ï¼šå°†é€—å·æ›¿æ¢ä¸ºç‚¹ï¼Œå¹¶ç§»é™¤æ‰€æœ‰ç©ºæ ¼
        time_str = time_str.replace(',', '.').replace(' ', '')
        
        # å¤„ç†å¸¦æ¯«ç§’çš„æ—¶é—´æ ¼å¼
        if '.' in time_str:
            # åˆ†å‰²å°æ—¶ã€åˆ†é’Ÿã€ç§’å’Œæ¯«ç§’
            parts = re.split(r'[:.]', time_str)
            
            if len(parts) >= 4:
                # HH:MM:SS.mmm æ ¼å¼
                hours = int(parts[0]) if parts[0] else 0
                minutes =int(parts[1]) if parts[1] else 0
                seconds = int(parts[2]) if parts[2] else 0
                milliseconds = int(parts[3].ljust(3, '0')[:3])  # ç¡®ä¿æ¯«ç§’éƒ¨åˆ†æœ‰ä¸‰ä½
                
                return hours * 3600000 + minutes * 60000 + seconds * 1000 + milliseconds
            elif len(parts) == 3:
                # MM:SS.mmm æ ¼å¼
                minutes = int(parts[0]) if parts[0] else 0
                seconds = int(parts[1]) if parts[1] else 0
                milliseconds = int(parts[2].ljust(3, '0')[:3])
                return minutes * 60000 + seconds * 1000 + milliseconds
            elif len(parts) == 2:
                # SS.mmm æ ¼å¼
                seconds = int(parts[0]) if parts[0] else 0
                milliseconds = int(parts[1].ljust(3, '0')[:3])
                return seconds * 1000 + milliseconds
            else:
                return 0
        else:
            # å¤„ç†ä¸å¸¦æ¯«ç§’çš„æ—¶é—´æ ¼å¼
            parts = time_str.split(':')
            if len(parts) == 3:  # HH:MM:SS
                hours, minutes, seconds = map(int, parts)
                return hours * 3600000 + minutes * 60000 + seconds * 1000
            elif len(parts) == 2:  # MM:SS
                minutes, seconds = map(int, parts)
                return minutes * 60000 + seconds * 1000
            elif len(parts) == 1:  # SS
                seconds = int(parts[0])
                return seconds * 1000
        
        return 0
    except Exception as e:
        print(f"æ—¶é—´è½¬æ¢é”™è¯¯: {time_str} - {str(e)}")
        return 0

def find_video_files(path):
    """æŸ¥æ‰¾æŒ‡å®šè·¯å¾„ä¸‹çš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶"""
    if not os.path.exists(path):
        return []
    
    files = []
    
    # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œç›´æ¥è¿”å›
    if os.path.isfile(path):
        _, ext = os.path.splitext(path)
        if ext.lower() in support_video_format:
            return [path]
        return []
    
    # å¦‚æœæ˜¯æ–‡ä»¶å¤¹ï¼Œé€’å½’æŸ¥æ‰¾
    for root, _, filenames in os.walk(path):
        for filename in filenames:
            _, ext = os.path.splitext(filename)
            if ext.lower() in support_video_format:
                files.append(os.path.join(root, filename))
    
    return files

def parse_srt_content(content):
    """è§£æSRTå­—å¹•å†…å®¹"""
    subtitles = []
    
    # åˆ†å‰²å­—å¹•å—
    blocks = re.split(r'\n\n+', content.strip())
    
    for block in blocks:
        lines = block.split('\n')
        if len(lines) >= 3:
            # ç¬¬ä¸€è¡Œæ˜¯åºå·ï¼Œç¬¬äºŒè¡Œæ˜¯æ—¶é—´èŒƒå›´
            time_line = lines[1]
            text = '\n'.join(lines[2:]).strip()
            
            if '-->' in time_line:
                start_str, end_str = time_line.split('-->', 1)
                start_str = start_str.strip()
                end_str = end_str.strip()
                
                start_ms = to_milliseconds(start_str)
                end_ms = to_milliseconds(end_str)
                
                if start_ms > 0 or end_ms > 0:
                    subtitles.append({
                        'start': start_ms,
                        'end': end_ms,
                        'text': text
                    })
    
    return subtitles

def match_subtitles_to_media(media_path, subtitle_files):
    """ä¸ºåª’ä½“æ–‡ä»¶åŒ¹é…æœ€åˆé€‚çš„å­—å¹•æ–‡ä»¶"""
    media_name = os.path.splitext(os.path.basename(media_path))[0]
    best_match = None
    best_score = 0
    
    for sub_file in subtitle_files:
        sub_name = os.path.splitext(os.path.basename(sub_file.name))[0]
        
        # è®¡ç®—æ–‡ä»¶åç›¸ä¼¼åº¦
        score = 0
        min_length = min(len(media_name), len(sub_name))
        for i in range(min_length):
            if media_name[i] == sub_name[i]:
                score += 1
        
        # å¦‚æœå½“å‰åŒ¹é…åº¦æ›´é«˜ï¼Œæ›´æ–°æœ€ä½³åŒ¹é…
        if score > best_score:
            best_match = sub_file
            best_score = score
    
    return best_match

def align_subtitles_with_asr(asr_sentences, subtitles):
    """å°†ASRç»“æœä¸å­—å¹•è¿›è¡Œå¯¹é½"""
    aligned_results = []
    
    # å¦‚æœæ²¡æœ‰å­—å¹•ï¼Œç›´æ¥è¿”å›ASRç»“æœ
    if not subtitles:
        return asr_sentences
    
    # å¯¹é½é€»è¾‘
    for sub in subtitles:
        best_match = None
        best_overlap = 0
        
        for asr_sentence in asr_sentences:
            # è®¡ç®—æ—¶é—´é‡å 
            overlap_start = max(sub['start'], asr_sentence['start'])
            overlap_end = min(sub['end'], asr_sentence['end'])
            overlap_duration = max(0, overlap_end - overlap_start)
            
            # è®¡ç®—é‡å æ¯”ä¾‹
            sub_duration = sub['end'] - sub['start']
            asr_duration = asr_sentence['end'] - asr_sentence['start']
            
            # æ›´æ–°æœ€ä½³åŒ¹é…
            if overlap_duration > best_overlap:
                best_match = asr_sentence
                best_overlap = overlap_duration
        
        # å¦‚æœæ‰¾åˆ°åŒ¹é…ï¼Œä½¿ç”¨å­—å¹•çš„æ—¶é—´æˆ³
        if best_match:
            aligned_results.append({
                'start': sub['start'],
                'end': sub['end'],
                'text': sub['text'],
                'spk': best_match['spk']
            })
        else:
            # å¦åˆ™ä¿ç•™ASRç»“æœ
            aligned_results.append(asr_sentence)
    
    return aligned_results

def detect_actual_speech(audio_path, asr_start, asr_end, silence_threshold=-40, min_silence_duration=200):
    """æ£€æµ‹éŸ³é¢‘ä¸­å®é™…è¯­éŸ³çš„èµ·æ­¢æ—¶é—´"""
    try:
        # æ‰©å±•æ£€æµ‹èŒƒå›´ï¼ˆå‰åå„åŠ 500æ¯«ç§’ï¼‰
        expand_ms = 500
        detection_start = max(0, asr_start - expand_ms)
        detection_end = asr_end + expand_ms
        
        # ä»åŸå§‹éŸ³é¢‘ä¸­æå–æ£€æµ‹ç‰‡æ®µ
        temp_wav = "temp_detection.wav"
        (
            ffmpeg.input(audio_path)
            .filter('atrim', start=detection_start/1000, end=detection_end/1000)
            .output(temp_wav, ac=1, ar=16000, acodec='pcm_s16le')
            .run(cmd=["ffmpeg", "-nostdin"], overwrite_output=True, quiet=True)
        )
        
        # åŠ è½½éŸ³é¢‘è¿›è¡Œåˆ†æ
        audio = AudioSegment.from_wav(temp_wav)
        
        # æ£€æµ‹éé™éŸ³éƒ¨åˆ†
        nonsilent_parts = detect_nonsilent(
            audio, 
            min_silence_len=min_silence_duration, 
            silence_thresh=silence_threshold
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_wav)
        
        if not nonsilent_parts:
            return asr_start, asr_end
        
        # è·å–ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªéé™éŸ³æ®µçš„èµ·æ­¢æ—¶é—´
        first_speech_start = nonsilent_parts[0][0]
        last_speech_end = nonsilent_parts[-1][1]
        
        # è®¡ç®—å®é™…åç§»é‡
        actual_start = detection_start + first_speech_start
        actual_end = detection_start + last_speech_end
        
        # åº”ç”¨è¾¹ç•Œå®‰å…¨æ‰©å±•
        start_buffer = 100  # å¼€å§‹å‰å¢åŠ 100ms
        end_buffer = 200    # ç»“æŸåå¢åŠ 200ms
        
        actual_start = max(0, actual_start - start_buffer)
        actual_end = actual_end + end_buffer
        
        # ç¡®ä¿è°ƒæ•´åçš„æ—¶é—´åœ¨åŸå§‹èŒƒå›´å†…
        actual_start = max(asr_start, actual_start)
        actual_end = min(asr_end, actual_end)
        
        return actual_start, actual_end
        
    except Exception as e:
        print(f"è¯­éŸ³è¾¹ç•Œæ£€æµ‹é”™è¯¯: {str(e)}")
        return asr_start, asr_end

def trans(file_paths, save_path, subtitle_files, threshold=10, min_duration=60, max_duration=10, 
          silence_threshold=-35, vad_refinement=True, progress=gr.Progress()):
    if model is None:
        return "é”™è¯¯ï¼šæ¨¡å‹ä»åœ¨åŠ è½½ä¸­ï¼Œè¯·ç¨åå†è¯•"
    
    if not file_paths:
        return "è¯·æŒ‡å®šè§†é¢‘æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹è·¯å¾„"
    
    if not save_path:
        return "è¯·æŒ‡å®šä¿å­˜è·¯å¾„"
    
    # æ”¶é›†æ‰€æœ‰è¦å¤„ç†çš„è§†é¢‘æ–‡ä»¶
    all_files = []
    for path in file_paths.split(';'):
        path = path.strip()
        if path:
            found_files = find_video_files(path)
            all_files.extend(found_files)
    
    if not all_files:
        return f"æœªæ‰¾åˆ°ä»»ä½•æ”¯æŒçš„è§†é¢‘æ–‡ä»¶ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(support_video_format)}"
    
    # å¤„ç†ä¸Šä¼ çš„å­—å¹•æ–‡ä»¶
    subtitle_contents = []
    if subtitle_files:
        for sub_file in subtitle_files:
            with open(sub_file.name, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                subtitle_contents.append(content)
    
    os.makedirs(save_path, exist_ok=True)
    date = datetime.now().strftime("%Y-%m-%d")
    final_save_path = os.path.join(save_path, date)
    os.makedirs(final_save_path, exist_ok=True)
    
    total_files = len(all_files)
    results = []
    min_duration_ms = min_duration * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
    
    for idx, video_path in enumerate(all_files):
        try:
            # å¤„ç†è¿›åº¦æ›´æ–°
            progress(idx / total_files, f"å¤„ç†æ–‡ä»¶ä¸­: {os.path.basename(video_path)}")
            
            video_name = os.path.splitext(os.path.basename(video_path))[0]
            _, video_extension = os.path.splitext(video_path)
            speaker_videos = {}  # æ¯ä¸ªè¯´è¯äººä½œä¸º keyï¼Œvalue ä¸ºåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­ä¸ºå½“å‰è¯´è¯äººå¯¹åº”çš„æ¯ä¸ªè§†é¢‘ç‰‡æ®µ
            speaker_valid_segments = {}  # è®°å½•æœ‰æ•ˆç‰‡æ®µæ•°é‡
            speaker_discarded_segments = {}  # è®°å½•èˆå¼ƒç‰‡æ®µæ•°é‡
            timing_adjustments = []  # è®°å½•æ—¶é—´è°ƒæ•´ä¿¡æ¯
            
            # è§†é¢‘é¢„å¤„ç†
            try:
                # å°è¯•ä¸ºå½“å‰åª’ä½“æ–‡ä»¶åŒ¹é…å­—å¹•
                matched_subtitles = []
                if subtitle_contents:
                    # å°è¯•æŸ¥æ‰¾åŒ¹é…çš„å­—å¹•å†…å®¹
                    for content in subtitle_contents:
                        subtitles = parse_srt_content(content)
                        # ç®€å•çš„åŒ¹é…ï¼šæ£€æŸ¥å­—å¹•ä¸­æ˜¯å¦åŒ…å«åª’ä½“æ–‡ä»¶å
                        if any(video_name.lower() in sub['text'].lower() for sub in subtitles):
                            matched_subtitles = subtitles
                            results.append(f"âœ… ä½¿ç”¨åŒ¹é…çš„å­—å¹•å†…å®¹ (åŸºäºæ–‡ä»¶å: {video_name})")
                            break
                    
                    # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå­—å¹•æ–‡ä»¶
                    if not matched_subtitles:
                        matched_subtitles = parse_srt_content(subtitle_contents[0])
                        results.append(f"â„¹â„¹ï¸ ä½¿ç”¨ç¬¬ä¸€ä¸ªå­—å¹•æ–‡ä»¶å†…å®¹")
                
                # è·å–è§†é¢‘æ€»æ—¶é•¿
                try:
                    probe = ffmpeg.probe(video_path)
                    audio_info = next((stream for stream in probe['streams'] if stream['codec_type'] == 'audio'), None)
                    if audio_info:
                        total_duration_ms = float(audio_info['duration']) * 1000
                    else:
                        # å¦‚æœæ²¡æœ‰éŸ³é¢‘æµï¼Œå¯èƒ½æ˜¯çº¯è§†é¢‘æ–‡ä»¶
                        total_duration_ms = 0
                        results.append(f"âš ï¸ {video_path} æœªæ£€æµ‹åˆ°éŸ³é¢‘æµ")
                except:
                    total_duration_ms = 0
                
                audio_bytes, _ = (
                    ffmpeg.input(video_path, threads=0, hwaccel='cuda' if device == "cuda" else None)
                    .output("-", format="wav", acodec="pcm_s16le", ac=1, ar=16000)
                    .run(cmd=["ffmpeg", "-nostdin"], capture_stdout=True, capture_stderr=True)
                )
                
                # ä½¿ç”¨é”ç¡®ä¿æ¨¡å‹å®‰å…¨è°ƒç”¨
                with threading.Lock():
                    res = model.generate(
                        input=audio_bytes, 
                        batch_size_s=300, 
                        is_final=True, 
                        sentence_timestamp=True, 
                        hotword=hotwords
                    )
                
                rec_result = res[0]
                asr_result_text = rec_result['text']
                
                if asr_result_text == '':
                    results.append(f"{video_name}: æœªæ£€æµ‹åˆ°è¯­éŸ³å†…å®¹")
                    continue
                
                # å¤„ç†ASRå¥å­ä¿¡æ¯
                asr_sentences = []
                for sentence in rec_result["sentence_info"]:
                    asr_sentences.append({
                        "start": sentence["start"],
                        "end": sentence["end"],
                        "text": sentence["text"],
                        "spk": sentence["spk"]
                    })
                
                # å°†ASRç»“æœä¸å­—å¹•å¯¹é½
                aligned_sentences = align_subtitles_with_asr(asr_sentences, matched_subtitles)
                
                # å¦‚æœæ²¡æœ‰å¯¹é½ç»“æœï¼Œä½¿ç”¨åŸå§‹ASRç»“æœ
                if not aligned_sentences:
                    aligned_sentences = asr_sentences
                    results.append("âš ï¸ æœªæ£€æµ‹åˆ°å­—å¹•ï¼Œä½¿ç”¨åŸå§‹ASRæ—¶é—´æˆ³")
                
                # å¤„ç†æ¯ä¸ªè¯´è¯äººçš„ç‰‡æ®µ
                for spk in set([s["spk"] for s in aligned_sentences]):
                    speaker_valid_segments[spk] = 0
                    speaker_discarded_segments[spk] = 0
                
                # ä¿å­˜å¯¹é½åçš„æ—¶é—´æˆ³ä¿¡æ¯
                timestamp_file = os.path.join(final_save_path, video_name, "timestamps.json")
                os.makedirs(os.path.dirname(timestamp_file), exist_ok=True)
                with open(timestamp_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "asr_sentences": asr_sentences,
                        "subtitles": matched_subtitles,
                        "aligned": aligned_sentences
                    }, f, ensure_ascii=False, indent=2)
                
                # å‰ªåˆ‡è§†é¢‘ç‰‡æ®µ
                for i, sentence in enumerate(aligned_sentences):
                    stn_txt = sentence['text']
                    start_ms = sentence['start']
                    end_ms = sentence['end']
                    spk = sentence['spk']
                    
                    # æ ¹æ®ASRç»“æœåˆ¤æ–­ï¼šå¦‚æœæ–‡æœ¬ä¸ºç©ºï¼Œåˆ™èˆå¼ƒè¯¥ç‰‡æ®µ
                    if stn_txt.strip() == '':
                        speaker_discarded_segments[spk] += 1
                        continue
                    
                    # åº”ç”¨VADä¼˜åŒ–ï¼šæ£€æµ‹å®é™…è¯­éŸ³è¾¹ç•Œ
                    if vad_refinement:
                        original_start = start_ms
                        original_end = end_ms
                        start_ms, end_ms = detect_actual_speech(
                            video_path, start_ms, end_ms, 
                            silence_threshold=silence_threshold
                        )
                        
                        # è®°å½•è°ƒæ•´ä¿¡æ¯
                        timing_adjustments.append({
                            "segment": i,
                            "original_start": original_start,
                            "original_end": original_end,
                            "adjusted_start": start_ms,
                            "adjusted_end": end_ms,
                            "start_diff": start_ms - original_start,
                            "end_diff": end_ms - original_end
                        })
                    
                    # æ ¼å¼åŒ–æ—¶é—´æˆ³
                    start_str = to_date(start_ms)
                    end_str = to_date(end_ms)
                    
                    # æ ¹æ®æ–‡ä»¶åå’Œ spk åˆ›å»ºç›®å½•
                    spk_save_path = os.path.join(final_save_path, video_name, str(spk))
                    os.makedirs(spk_save_path, exist_ok=True)
                    
                    # æ–‡æœ¬è®°å½•
                    spk_txt_file = os.path.join(final_save_path, video_name, f'spk{spk}.txt')
                    with open(spk_txt_file, 'a', encoding='utf-8') as f:
                        f.write(f"{start_str} --> {end_str}\n{stn_txt}\n\n")
                    
                    # å¤„ç†è§†é¢‘ç‰‡æ®µ
                    final_save_file = os.path.join(spk_save_path, f"{i}.mp4")
                    
                    try:
                        (
                            ffmpeg.input(video_path, threads=0, ss=start_ms/1000, to=end_ms/1000, hwaccel='cuda' if device == "cuda" else None)
                            .output(final_save_file, vcodec='libx264', crf=23, acodec='aac', ab='128k')
                            .run(cmd=["ffmpeg", "-nostdin"], overwrite_output=True, capture_stdout=True, capture_stderr=True)
                        )
                    except ffmpeg.Error as e:
                        error_msg = e.stderr.decode('utf-8') if e.stderr else str(e)
                        results.append(f"{video_name}: å‰ªåˆ‡é”™è¯¯ - {error_msg}")
                        continue
                    
                    # è®°å½•è¯¥ç‰‡æ®µä¸ºæœ‰æ•ˆç‰‡æ®µ
                    speaker_valid_segments[spk] += 1
                    
                    # è®°å½•è¯´è¯äººå’Œå¯¹åº”çš„è§†é¢‘ç‰‡æ®µ
                    if spk not in speaker_videos:
                        speaker_videos[spk] = []
                    speaker_videos[spk].append({
                        'file': final_save_file, 
                        'video_name': video_name,
                        'start': start_ms,
                        'end': end_ms
                    })
                
                # åˆå¹¶æ¯ä¸ªè¯´è¯äººçš„è§†é¢‘ç‰‡æ®µï¼ˆå¢åŠ æ—¶é•¿è¿‡æ»¤ï¼‰
                for spk, video_segments in speaker_videos.items():
                    if not video_segments:
                        continue
                    
                    # è®¡ç®—è¯´è¯äººæ€»æ—¶é•¿
                    total_duration = 0
                    for seg in video_segments:
                        total_duration += (seg['end'] - seg['start'])
                    
                    # è½¬æ¢ä¸ºç§’
                    total_duration_sec = total_duration / 1000
                    
                    # å¦‚æœæ€»æ—¶é•¿ä¸è¶³é˜ˆå€¼ï¼Œè·³è¿‡åˆå¹¶
                    if total_duration_sec < min_duration:
                        results.append(f"è·³è¿‡è¯´è¯äºº {spk}ï¼Œæ€»æ—¶é•¿ {total_duration_sec:.1f}ç§’ä¸è¶³{min_duration}ç§’")
                        continue
                    
                    # è¾“å‡ºåˆå¹¶è§†é¢‘æ–‡ä»¶ï¼ˆMP4æ ¼å¼ï¼‰
                    output_file = os.path.join(final_save_path, video_name, f"{spk}.mp4")
                    
                    # æ„å»ºåˆå¹¶å‘½ä»¤
                    try:
                        # åˆ›å»ºæ–‡ä»¶åˆ—è¡¨
                        concat_list = os.path.join(spk_save_path, "concat_list.txt")
                        with open(concat_list, 'w', encoding='utf-8') as f:
                            for seg in video_segments:
                                f.write(f"file '{seg['file']}'\n")
                        
                        # ä½¿ç”¨ffmpegåˆå¹¶è§†é¢‘
                        (
                            ffmpeg.input(concat_list, format='concat', safe=0)
                            .output(output_file, c='copy')
                            .run(cmd=["ffmpeg", "-nostdin"], overwrite_output=True)
                        )
                        
                        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                        os.remove(concat_list)
                        
                        # æ·»åŠ èˆå¼ƒä¿¡æ¯
                        discarded_info = f" (èˆå¼ƒ{speaker_discarded_segments[spk]}ä¸ªç©ºæ–‡æœ¬ç‰‡æ®µ)"
                        results.append(f"âœ… åˆå¹¶å®Œæˆ: {os.path.basename(output_file)}ï¼Œæ—¶é•¿ {total_duration_sec:.1f}ç§’{discarded_info}")
                    except Exception as e:
                        results.append(f"âŒâŒ åˆå¹¶é”™è¯¯: {str(e)}")
                
                # æ·»åŠ æ—¶é—´è°ƒæ•´æŠ¥å‘Š
                if vad_refinement and timing_adjustments:
                    results.append("\nğŸ”§ğŸ”§ æ—¶é—´æˆ³è°ƒæ•´æŠ¥å‘Š:")
                    total_start_diff = 0
                    total_end_diff = 0
                    
                    for adj in timing_adjustments:
                        start_str = to_date(adj['original_start'])
                        end_str = to_date(adj['original_end'])
                        
                        results.append(
                            f"  ç‰‡æ®µ {adj['segment']}: {start_str} --> {end_str} "
                            f"| è°ƒæ•´: å¼€å§‹ {adj['start_diff']:+d}ms, ç»“æŸ {adj['end_diff']:+d}ms"
                        )
                        total_start_diff += adj['start_diff']
                        total_end_diff += adj['end_diff']
                    
                    avg_start_diff = total_start_diff / len(timing_adjustments)
                    avg_end_diff = total_end_diff / len(timing_adjustments)
                    results.append(
                        f"  å¹³å‡è°ƒæ•´: å¼€å§‹ {avg_start_diff:.1f}ms, ç»“æŸ {avg_end_diff:.1f}ms"
                    )
                
                # æ·»åŠ èˆå¼ƒæ€»ç»“ä¿¡æ¯
                for spk in set([s["spk"] for s in aligned_sentences]):
                    if spk in speaker_videos:
                        summary = f"è¯´è¯äºº {spk}: ä¿ç•™{speaker_valid_segments[spk]}ç‰‡æ®µï¼Œèˆå¼ƒ{speaker_discarded_segments[spk]}ä¸ªç©ºæ–‡æœ¬ç‰‡æ®µ"
                        results.append(summary)
                
                results.append(f"âœ… {video_name}: å¤„ç†å®Œæˆï¼Œä¿å­˜è‡³ {os.path.join(final_save_path, video_name)}")
                
            except Exception as e:
                results.append(f"âŒâŒ {video_name}: å¤„ç†å¤±è´¥ - {str(e)}")
                import traceback
                traceback.print_exc()
            
        except Exception as e:
            results.append(f"âŒâŒ æ–‡ä»¶å¤„ç†å¼‚å¸¸: {str(e)}")
            import traceback
            traceback.print_exc()
    
    return "\n".join(results)

# Gradio UI
with gr.Blocks(title="è§†é¢‘è¯´è¯äººåˆ†ç¦»å·¥å…·") as demo:
    gr.Markdown("## ğŸ¬ è§†é¢‘è¯´è¯äººåˆ†ç¦»å·¥å…·")
    gr.Markdown("å¤„ç†è§†é¢‘æ–‡ä»¶å¹¶åˆ†ç¦»ä¸åŒè¯´è¯äººçš„ç‰‡æ®µï¼Œè¾“å‡ºåˆ†è§’è‰²çš„åˆå¹¶è§†é¢‘")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("### è¾“å…¥è®¾ç½®")
            file_paths = gr.Textbox(
                label="è§†é¢‘æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹è·¯å¾„",
                placeholder="è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„æˆ–åŒ…å«è§†é¢‘çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¤šä¸ªç”¨åˆ†å·;åˆ†éš”ï¼‰",
                value="/root/autodl-tmp/input"
            )
            
            subtitle_files = gr.File(
                label="ä¸Šä¼ å­—å¹•æ–‡ä»¶ (SRTæ ¼å¼ï¼Œå¯ä¸Šä¼ å¤šä¸ª)",
                file_count="multiple",
                file_types=[".srt"],
            )
            gr.Markdown("*ç³»ç»Ÿä¼šè‡ªåŠ¨åŒ¹é…å¯¹åº”çš„åª’ä½“æ–‡ä»¶*", elem_classes="caption")
            
            save_path = gr.Textbox(
                label="ä¿å­˜è·¯å¾„",
                placeholder="è¾“å…¥ä¿å­˜ç»“æœçš„ç›®å½•è·¯å¾„",
                value="/root/autodl-tmp/output"
            )
            
            with gr.Row():
                example_files = gr.Examples(
                    examples=[
                        ["/root/autodl-tmp/video1.mp4"],
                        ["/root/autodl-tmp/video_folder"],
                        ["/root/autodl-tmp/video1.mp4;/root/autodl-tmp/video2.mkv"]
                    ],
                    inputs=[file_paths],
                    label="ç¤ºä¾‹è·¯å¾„"
                )
            
            with gr.Row():
                threshold = gr.Slider(
                    minimum=5,
                    maximum=30,
                    value=10,
                    step=1,
                    label="åˆå¹¶ç›¸åŒè¯´è¯äººé˜ˆå€¼",
                    info="æ•°å€¼è¶Šå°åˆ†å‰²è¶Šç»†"
                )
                min_duration = gr.Slider(
                    minimum=5,
                    maximum=300,
                    value=60,
                    step=5,
                    label="æœ€çŸ­åˆå¹¶æ—¶é•¿ï¼ˆç§’ï¼‰",
                    info="æ€»æ—¶é•¿ä½äºæ­¤å€¼çš„è¯´è¯äººå°†è¢«èˆå¼ƒ"
                )
                max_duration = gr.Slider(
                    minimum=5,
                    maximum=20,
                    value=10,
                    step=5,
                    label="æœ€å¤§ç‰‡æ®µæ—¶é•¿ï¼ˆç§’ï¼‰",
                    info="è¶…è¿‡æ­¤æ—¶é•¿çš„ç‰‡æ®µä¼šè¢«è‡ªåŠ¨åˆ†å‰²"
                )
                silence_threshold = gr.Slider(
                    minimum=-60,
                    maximum=-20,
                    value=-35,
                    step=1,
                    label="VADæ£€æµ‹é˜ˆå€¼(dB)",
                    info="å€¼è¶Šä½è¶Šå®¹æ˜“æ£€æµ‹åˆ°è¯­éŸ³"
                )
            
            with gr.Row():
                vad_refinement = gr.Checkbox(
                    value=True,
                    label="å¯ç”¨ç²¾å‡†æ—¶é—´æˆ³ä¼˜åŒ–",
                    info="æ ¡æ­£ASRä¸éŸ³é¢‘å®é™…ç»“æŸç‚¹åå·®"
                )
                submit_btn = gr.Button("å¼€å§‹å¤„ç†", variant="primary")
            
            with gr.Accordion("æ–‡ä»¶è·¯å¾„å¸®åŠ©", open=False):
                gr.Markdown("""
                - æ”¯æŒç›´æ¥è¾“å…¥æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¤¹è·¯å¾„
                - å¤šä¸ªè·¯å¾„ç”¨åˆ†å·(;)åˆ†éš”
                - æ”¯æŒçš„è§†é¢‘æ ¼å¼: .mp4, .avi, .mov, .mkv, .flv, .ts
                - æ”¯æŒçš„å­—å¹•æ ¼å¼: .srt
                - æ–‡ä»¶å¤¹è·¯å¾„ä¼šè‡ªåŠ¨æœç´¢æ‰€æœ‰æ”¯æŒçš„è§†é¢‘æ–‡ä»¶
                """)
        
        with gr.Column():
            output_result = gr.Textbox(label="å¤„ç†ç»“æœ", lines=20, interactive=False)
            gr.Markdown("### ä½¿ç”¨è¯´æ˜")
            gr.Markdown("""
            **è§†é¢‘è¯´è¯äººåˆ†ç¦»æŠ€æœ¯ï¼š**
            1. ç”¨æˆ·ä¸Šä¼ SRTå­—å¹•æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            2. ç³»ç»Ÿå°†ASRè¯†åˆ«ç»“æœä¸å­—å¹•æ—¶é—´æˆ³è¿›è¡Œå¯¹é½
            3. åŸºäºç²¾å‡†çš„æ—¶é—´æˆ³æå–å„è§’è‰²è§†é¢‘ç‰‡æ®µ
            4. ä½¿ç”¨VADæŠ€æœ¯æ ¡æ­£æ—¶é—´æˆ³åå·®
            
            **å¤„ç†æµç¨‹ï¼š**
            1. åœ¨å®ä¾‹ä¸Šå‡†å¤‡è§†é¢‘æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹
            2. æŒ‡å®šæ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¤¹è·¯å¾„
            3. ä¸Šä¼ å­—å¹•æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            4. æŒ‡å®šä¿å­˜è·¯å¾„
            5. è°ƒæ•´å‚æ•°ï¼š
               - **åˆå¹¶é˜ˆå€¼**ï¼šæ§åˆ¶ç›¸é‚»ç›¸åŒè¯´è¯äººç‰‡æ®µçš„åˆå¹¶ç¨‹åº¦
               - **æœ€çŸ­æ—¶é•¿**ï¼šèˆå¼ƒæ€»æ—¶é•¿ä½äºæ­¤å€¼çš„è¯´è¯äººè¾“å‡º
               - **æœ€å¤§æ—¶é•¿**ï¼šè‡ªåŠ¨åˆ†å‰²è¶…è¿‡æ­¤æ—¶é•¿çš„ç‰‡æ®µ
               - **VADé˜ˆå€¼**ï¼šè¯­éŸ³æ´»åŠ¨æ£€æµ‹çš„çµæ•åº¦
               - **å¯ç”¨ç²¾å‡†æ—¶é—´æˆ³**ï¼šæ ¡æ­£æ—¶é—´æˆ³åå·®ï¼ˆæ¨èï¼‰
            6. ç‚¹å‡»å¼€å§‹å¤„ç†æŒ‰é’®
            7. ç»“æœå°†ä¿å­˜åœ¨æŒ‡å®šç›®å½•ä¸‹çš„æ—¥æœŸæ–‡ä»¶å¤¹ä¸­ï¼ŒåŒ…å«ï¼š
               - å„è§’è‰²çš„è§†é¢‘ç‰‡æ®µ
               - åˆå¹¶åçš„å®Œæ•´è§’è‰²è§†é¢‘
               - æ—¶é—´æˆ³å¯¹é½ä¿¡æ¯æ–‡ä»¶
            """)
    
    submit_btn.click(
        fn=trans,
        inputs=[file_paths, save_path, subtitle_files, threshold, min_duration, max_duration, silence_threshold, vad_refinement],
        outputs=output_result
    )

if __name__ == "__main__":
    # åœ¨Autodlä¸Šå»ºè®®ä½¿ç”¨share=Trueå¼€å¯å…¬ç½‘è®¿é—®
    demo.launch(
        server_name="0.0.0.0",
        server_port=6006,
        share=False,
        show_error=True
    )